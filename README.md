<div align="center">
  <h1>ğŸ™ï¸ microWakeWord Nvidia Trainer & Recorder</h1>
  <img width="1002" height="593" alt="Screenshot 2026-01-18 at 8 13 35â€¯AM" src="https://github.com/user-attachments/assets/e1411d8a-8638-4df8-992b-09a46c6e5ddc" />
</div>

Train **microWakeWord** detection models using a simple **web-based recorder + trainer UI**, packaged in a Docker container.

No Jupyter notebooks required. No manual cell execution. Just record your voice (optional) and train.

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Pull the Docker Image

```bash
docker pull ghcr.io/tatertotterson/microwakeword:latest
```

---

### 2ï¸âƒ£ Run the Container

```bash
docker run --rm -it \
  --gpus all \
  -p 8888:8888 \
  -v $(pwd):/data \
  ghcr.io/tatertotterson/microwakeword:latest
```

**What these flags do:**
- `--gpus all` â†’ Enables GPU acceleration  
- `-p 8888:8888` â†’ Exposes the Recorder + Trainer WebUI  
- `-v $(pwd):/data` â†’ Persists all models, datasets, and cache  

---

### 3ï¸âƒ£ Open the Recorder WebUI

Open your browser and go to:

ğŸ‘‰ **http://localhost:8888**

Youâ€™ll see the **microWakeWord Recorder & Trainer UI**.

---

## ğŸ¤ Recording Voice Samples (Optional)

Personal voice recordings are **optional**.

- You may **record your own voice** for better accuracy  
- Or simply **click â€œTrainâ€ without recording anything**

If no recordings are present, training will proceed using **synthetic TTS samples only**.

### Remote systems (important)
If you are running this on a **remote PC / server**, browser-based recording will not work unless:
- You use a **reverse proxy** (HTTPS + mic permissions), **or**
- You access the UI via **localhost** on the same machine

Training itself works fine remotely â€” only recording requires local microphone access.

---

## ğŸ§  Training Behavior (Important Notes)

### â¬ First training run
The **first time you click Train**, the system will download **large training datasets** (background noise, speech corpora, etc.).

- This can take **several minutes**
- This happens **only once**
- Data is cached inside `/data`

You **will NOT need to download these again** unless you delete `/data`.

---

### ğŸ” Re-training is safe and incremental

- You can train **multiple wake words** back-to-back
- You do **NOT** need to clear any folders between runs
- Old models are preserved in timestamped output directories
- All required cleanup and reuse logic is handled automatically

---

## ğŸ“¦ Output Files

When training completes, youâ€™ll get:
- `<wake_word>.tflite` â€“ quantized streaming model  
- `<wake_word>.json` â€“ ESPHome-compatible metadata  

Both are saved under:

```text
/data/output/
```

Each run is placed in its own timestamped folder.

---

## ğŸ¤ Optional: Personal Voice Samples (Advanced)

If you record personal samples:
- They are automatically augmented
- They are **up-weighted during training**
- This significantly improves real-world accuracy

No configuration required â€” detection is automatic.

---

## ğŸ”„ Resetting Everything (Optional)

If you want a **completely clean slate**:

Delete the /data folder

Then restart the container.

âš ï¸ This will:
- Remove cached datasets
- Require re-downloading training data
- Delete trained models

---

## ğŸ™Œ Credits

Built on top of the excellent  
**https://github.com/kahrendt/microWakeWord**

Huge thanks to the original authors â¤ï¸
